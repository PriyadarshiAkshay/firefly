{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from transitfit import split_lightcurve_file, run_retrieval\n",
    "from astroquery.mast import Observations as obs\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "from astropy.config import set_temp_cache\n",
    "from astropy.table import Table\n",
    "from pandas import DataFrame, read_csv, Categorical\n",
    "from shutil import rmtree, make_archive, copy\n",
    "from natsort import natsorted\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "The backend for auto_retrieval.\n",
    "\n",
    "@author: Steven Charles-Mindoza\n",
    "\"\"\"\n",
    "\n",
    "from firefly._archive import priors, _tic, _load_csv, _search\n",
    "from firefly._plot import oc_fold, density_scatter\n",
    "\n",
    "from rich import print\n",
    "from transitfit import split_lightcurve_file, run_retrieval\n",
    "from astroquery.mast import Observations as obs\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "from astropy.config import set_temp_cache\n",
    "from astropy.table import Table\n",
    "from pandas import DataFrame, read_csv, Categorical\n",
    "from shutil import rmtree, make_archive, copy\n",
    "from natsort import natsorted\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def _TESS_filter():\n",
    "    here = os.path.dirname(os.path.abspath(__file__))\n",
    "    os.makedirs('firefly/data', exist_ok=True)\n",
    "    tess_filter_path = 'firefly/data/TESS_filter_path.csv'\n",
    "    try:\n",
    "        os.remove(tess_filter_path)\n",
    "    except:\n",
    "        pass\n",
    "    tess_filter = f'{here}/data/Filters/TESS_filter.csv'\n",
    "    cols = ['filter_idx', 'low_wl', 'high_wl']\n",
    "    df = DataFrame(columns=cols)\n",
    "    df = df.append([{'filter_idx': 0,\n",
    "                     'low_wl': tess_filter}], ignore_index=True)\n",
    "    df.to_csv(tess_filter_path, index=False, header=True)\n",
    "\n",
    "\n",
    "def _alias(exoplanet):\n",
    "    alias = read_csv('https://exoplanetarchive.ipac.caltech.edu/cgi-bin/' +\\\n",
    "        f'nstedAPI/nph-nstedAPI?table=aliastable&objname={exoplanet[:-2]}')\n",
    "    check_wasp = alias['aliasdis'][1]\n",
    "    if '1SWASP' in check_wasp:\n",
    "        return check_wasp\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def mast(exoplanet):\n",
    "    _load_csv()\n",
    "    highest, ratios = _search(exoplanet)\n",
    "    exoplanet = highest[0]\n",
    "    tic_id = _tic(exoplanet).replace('TIC ', '')\n",
    "    print(f'\\nSearching MAST for {exoplanet} (TIC {tic_id}).')\n",
    "    search = obs.query_criteria(dataproduct_type=['timeseries'],\n",
    "                                project='TESS',\n",
    "                                target_name=tic_id\n",
    "                                ).to_pandas()\n",
    "    data = search[['obs_id', 'target_name', 't_exptime',\n",
    "               'provenance_name', 'project']]\n",
    "    data = data.rename(columns={\"obs_id\": \"Product\",\n",
    "                                \"target_name\": \"TIC ID\",\n",
    "                                \"t_exptime\": \"Cadence\",\n",
    "                                \"provenance_name\": \"HLSP\",\n",
    "                                \"project\": \"Mission\"})\n",
    "    data['Product'] = \\\n",
    "                Categorical(data['Product'],\n",
    "                ordered=True,\n",
    "                categories=natsorted(data['Product'].unique()))\n",
    "    data = data.sort_values('Product')\n",
    "    print(f'\\nQuery from MAST returned {len(search)} '\n",
    "          f'data products for {exoplanet} (TIC {tic_id}).\\n')\n",
    "    return print(tabulate(data, tablefmt='psql', showindex=False, headers='keys'))\n",
    "\n",
    "\n",
    "def discover():\n",
    "    here = os.path.dirname(os.path.abspath(__file__))\n",
    "    os.makedirs('firefly/discover', exist_ok=True)\n",
    "    upper_ecliptic = f'{here}/data/Candidates/19_sector_candidates.csv'\n",
    "    #upper_ecliptic = '19_sector_candidates.csv'\n",
    "    tic_ids = read_csv(upper_ecliptic)['TIC ID'].tolist()\n",
    "    for i, tic_id in enumerate(tic_ids):\n",
    "        _discover_search(tic_id)\n",
    "        \n",
    "        \n",
    "def _discover_search(tic_id):\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # MAST Search\n",
    "    search = obs.query_criteria(dataproduct_type=['timeseries'],\n",
    "                                project='TESS',\n",
    "                                provenance_name='SPOC',\n",
    "                                t_exptime=120,\n",
    "                                target_name=tic_id\n",
    "                                ).to_pandas()\n",
    "    data = search[['obs_id', 'target_name', 'dataURL', 't_exptime',\n",
    "                   'provenance_name', 'project']]\n",
    "    data['dataURL'] = ['https://mast.stsci.edu/api/v0.1/Download/file/?uri=' +\\\n",
    "                         data['dataURL'][i] for i in range(len(search))]\n",
    "    # data = data[data['t_exptime'].isin(cadence)]\n",
    "    data = data[~data.dataURL.str.endswith('_dvt.fits')].reset_index(drop=True)\n",
    "    provenance_name = data['provenance_name'].tolist()\n",
    "    lc_links = data['dataURL'].tolist()\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Extract Time series\n",
    "    os.makedirs('firefly/discover', exist_ok=True)\n",
    "    csv_in_dir = []\n",
    "    print(f'Fitting TIC {tic_id}')\n",
    "    for j, fitsfile in enumerate(lc_links):\n",
    "        with fits.open(fitsfile, cache=False) as TESS_fits:\n",
    "            time = TESS_fits[1].data['TIME'] + 2457000\n",
    "            flux = TESS_fits[1].data['PDCSAP_FLUX']\n",
    "            flux_err = TESS_fits[1].data['PDCSAP_FLUX_ERR']\n",
    "            \n",
    "        write_dict = []\n",
    "        for i in range(len(time)):\n",
    "            write_dict.append({'Time': time[i], 'Flux': flux[i],\n",
    "                               'Flux err': flux_err[i]})\n",
    "        source = f'firefly/discover/{tic_id}'\n",
    "        mast_name = data['obs_id'][j]\n",
    "        os.makedirs(f'{source}', exist_ok=True)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Extract all light curves to a single csv file\n",
    "        csv_name = f'{source}/{mast_name}/{mast_name}.csv'\n",
    "        write_dict = {'Time': time, 'Flux': flux, 'Flux err': flux_err}\n",
    "        df = DataFrame(write_dict)\n",
    "        df.to_csv(csv_name, index=False, na_rep='nan')\n",
    "        csv_in_dir.append(f'{os.getcwd()}/{csv_name}')\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Plot the final output\n",
    "    for k, csv in enumerate(csv_in_dir):\n",
    "        lc_plot(csv)\n",
    "\n",
    "def clip(df):\n",
    "    from lightkurve import LightCurve\n",
    "    from astropy.stats.funcs import mad_std\n",
    "    time = df['TIME']\n",
    "    flux = df['PDCSAP_FLUX']\n",
    "    flux_err = df['PDCSAP_FLUX_ERR']\n",
    "    lc = LightCurve(time, flux, flux_err)\n",
    "    lc = lc.remove_outliers(sigma_upper=6, sigma_lower=50, stdfunc=mad_std)\n",
    "    df_new = DataFrame()\n",
    "    df_new['TIME'] = lc.time\n",
    "    df_new['PDCSAP_FLUX'] = lc.flux\n",
    "    df_new['PDCSAP_FLUX_ERR'] = lc.flux_err\n",
    "    print(f'Sigma clipped {len(df)-len(df_new)} cadences.')\n",
    "    return df_new\n",
    "\n",
    "def _fits(exoplanet,\n",
    "          exo_folder,\n",
    "          cache,\n",
    "          hlsp,\n",
    "          cadence,\n",
    "          bitmask\n",
    "):\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # MAST Search\n",
    "    tic_id = _tic(exoplanet).replace('TIC ', '')\n",
    "    print(f'\\nSearching MAST for {exoplanet} (TIC {tic_id}).')\n",
    "    search = obs.query_criteria(dataproduct_type=['timeseries'],\n",
    "                                project='TESS',\n",
    "                                provenance_name=hlsp,\n",
    "                                t_exptime=cadence,\n",
    "                                target_name=tic_id\n",
    "                                ).to_pandas()\n",
    "    if len(search)==0:\n",
    "        rmtree(exo_folder)\n",
    "        print(f'Search result contains no data products for {exoplanet}.')\n",
    "        sys.exit(f'Search result contains no data products for {exoplanet}.')\n",
    "    data = search[['obs_id', 'target_name', 'dataURL', 't_exptime',\n",
    "                   'provenance_name', 'project', 'sequence_number']]\n",
    "    data['dataURL'] = ['https://mast.stsci.edu/api/v0.1/Download/file/?uri=' +\\\n",
    "                         data['dataURL'][i] for i in range(len(search))]\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Dataframe Checks\n",
    "    data = data[data['t_exptime']==cadence]\n",
    "    data = data[~data.dataURL.str.endswith('_dvt.fits')].reset_index(drop=True)\n",
    "    provenance_name = data['provenance_name'].tolist()\n",
    "    lc_links = data['dataURL'].tolist()\n",
    "    print(f'\\nQuery from MAST returned {len(lc_links)} '\n",
    "          f'data products for {exoplanet} (TIC {tic_id}).\\n')\n",
    "    sector_list = natsorted(data['sequence_number'].values.tolist())\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Print search result\n",
    "    show = data[['obs_id', 'target_name', 't_exptime',\n",
    "                   'provenance_name', 'project']]\n",
    "    show = show.rename(columns={\"obs_id\": \"Product\",\n",
    "                                \"target_name\": \"TIC ID\",\n",
    "                                \"t_exptime\": \"Cadence\",\n",
    "                                \"provenance_name\": \"HLSP\",\n",
    "                                \"project\": \"Mission\"})\n",
    "    show['Product'] = \\\n",
    "            Categorical(show['Product'],\n",
    "            ordered=True,\n",
    "            categories=natsorted(show['Product'].unique()))\n",
    "    show = show.sort_values('Product')\n",
    "    print(tabulate(show, tablefmt='psql', showindex=False, headers='keys'))\n",
    "    print()\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Extract Time series\n",
    "    csv_in_dir = []\n",
    "    os.makedirs('firefly/data/cache', exist_ok=True)\n",
    "    for j, fitsfile in enumerate(lc_links):\n",
    "        TESS_fits = None\n",
    "        while TESS_fits is None:\n",
    "            try:\n",
    "                with set_temp_cache('firefly/data/cache'):\n",
    "                    TESS_fits = Table.read(fitsfile, cache=cache)\n",
    "            except:\n",
    "                pass\n",
    "            if TESS_fits is not None:\n",
    "                pass\n",
    "        source = f'{exo_folder}/mastDownload'\n",
    "        mast_name = data['obs_id'][j]\n",
    "        os.makedirs(f'{source}/{mast_name}', exist_ok=True)\n",
    "        TESS_fits.write(f'{source}/{mast_name}/{mast_name}.fits',\n",
    "                          overwrite=True)\n",
    "        TESS_fits = TESS_fits.to_pandas()\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        # Quality Masks\n",
    "        AttitudeTweak = 1\n",
    "        SafeMode = 2\n",
    "        CoarsePoint = 4\n",
    "        EarthPoint = 8\n",
    "        Argabrightening = 16\n",
    "        Desat = 32\n",
    "        ApertureCosmic = 64\n",
    "        ManualExclude = 128\n",
    "        Discontinuity = 256\n",
    "        ImpulsiveOutlier = 512\n",
    "        CollateralCosmic = 1024\n",
    "        #: The first stray light flag is set manually by MIT based on visual inspection.\n",
    "        Straylight = 2048\n",
    "        #: The second stray light flag is set automatically by Ames/SPOC based on background level thresholds.\n",
    "        Straylight2 = 4096\n",
    "        PlanetSearchExclude = 8192\n",
    "        BadCalibrationExclude = 16384\n",
    "        IsuffTargetsErrorCorr = 32768\n",
    "        DEFAULT_BITMASK = [\n",
    "            AttitudeTweak, SafeMode, CoarsePoint, EarthPoint, Desat, ManualExclude,\n",
    "            ImpulsiveOutlier, Argabrightening, BadCalibrationExclude#, Straylight2\n",
    "        ]\n",
    "        HARD_BITMASK = [\n",
    "            AttitudeTweak, SafeMode, CoarsePoint, EarthPoint, Desat, ManualExclude,\n",
    "            ImpulsiveOutlier, Argabrightening, BadCalibrationExclude, Straylight2\n",
    "        ]\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Extract all light curves to a single csv file\n",
    "        TESS_fits['TIME'] = TESS_fits['TIME'] + 2457000\n",
    "        TESS_fits['PDCSAP_FLUX'] = TESS_fits['PDCSAP_FLUX'] + 10000\n",
    "        csv_name = f'{source}/{mast_name}/{mast_name}.csv'\n",
    "        _df = TESS_fits[['TIME', 'PDCSAP_FLUX', 'PDCSAP_FLUX_ERR', 'QUALITY']]\n",
    "        if bitmask=='default':\n",
    "            df = _df[~_df['QUALITY'].isin(DEFAULT_BITMASK)].drop('QUALITY', axis=1)\n",
    "            print(f'Removed {len(_df)-len(df)} bad cadences.')\n",
    "        elif bitmask=='hard':\n",
    "            df = _df[~_df['QUALITY'].isin(HARD_BITMASK)].drop('QUALITY', axis=1)\n",
    "            print(f'Removed {len(_df)-len(df)} bad cadences.')\n",
    "            # df = clip(df)\n",
    "        else:\n",
    "            df = _df.drop('QUALITY', axis=1)\n",
    "        df = df.rename(columns = {'TIME':'Time',\n",
    "                                  'PDCSAP_FLUX':'Flux',\n",
    "                                  'PDCSAP_FLUX_ERR':'Flux err'})\n",
    "        df.to_csv(csv_name, index=False, na_rep='nan')\n",
    "        csv_in_dir.append(f'{os.getcwd()}/{csv_name}')\n",
    "    csv_in_dir = natsorted(csv_in_dir)\n",
    "    print('\\nSplitting up the lightcurves into seperate epochs:\\n')\n",
    "    # csv_in_dir = []\n",
    "    # for r, d, f in os.walk(source):\n",
    "    #     for item in f:\n",
    "    #         if '.csv' in item:\n",
    "    #             csv_in_dir.append(os.path.join(r, item))\n",
    "    return csv_in_dir, sector_list\n",
    "    \n",
    "\n",
    "def _retrieval(\n",
    "        # Firefly Interface\n",
    "        exoplanet,\n",
    "        archive='eu',\n",
    "        curve_sample=1,\n",
    "        clean=False,\n",
    "        cache=False,\n",
    "        auto=True,\n",
    "        # MAST Search\n",
    "        hlsp=['SPOC'],\n",
    "        cadence=120,\n",
    "        bitmask='default',\n",
    "        # TransitFit Variables\n",
    "        walks=25,\n",
    "        slices=5,\n",
    "        cutoff=0.25,\n",
    "        window=2.5,\n",
    "        nlive=300,\n",
    "        fit_ttv=False,\n",
    "        detrending_list=[['nth order', 2]],\n",
    "        dynesty_sample='auto',\n",
    "        fitting_mode='auto',\n",
    "        limb_darkening_model='quadratic',\n",
    "        ld_fit_method='independent',\n",
    "        max_batch_parameters=25,\n",
    "        batch_overlap=2,\n",
    "        dlogz=None,\n",
    "        maxiter=None,\n",
    "        maxcall=None,\n",
    "        dynesty_bounding='multi',\n",
    "        normalise=True,\n",
    "        detrend=True,\n",
    "        detrending_limits=[[-10,10]],\n",
    "        # Plotting\n",
    "        plot=True,\n",
    "        marker_color='dimgray',\n",
    "        line_color='black',\n",
    "        bin_data=True,\n",
    "        binned_color='red',\n",
    "        nprocs=1,\n",
    "):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Filter Setup\n",
    "    exo_folder = f'firefly/{exoplanet}'\n",
    "    os.makedirs(exo_folder, exist_ok=True)\n",
    "    _TESS_filter()\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Download Archive\n",
    "    if fit_ttv==True:\n",
    "        archive = 'spearnet'\n",
    "    if archive=='nasa':\n",
    "        host_T, host_z, host_r, host_logg, t0, P, t14, repack, ra, dec, dist, t_mag = \\\n",
    "            priors(exoplanet, archive=archive, save=True, user=False, auto=auto,\n",
    "                    fit_ttv=fit_ttv)\n",
    "    else:\n",
    "        host_T, host_z, host_r, host_logg, t0, P, t14, repack = \\\n",
    "            priors(exoplanet, archive=archive, save=True, user=False, auto=auto,\n",
    "                    fit_ttv=fit_ttv)\n",
    "    if auto==False:\n",
    "        answer = ''\n",
    "        while answer!='y':\n",
    "            answer = input('Modify your priors file, type y to proceed. ')\n",
    "            if answer=='q':\n",
    "                sys.exit('Exiting..')\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Split the Light curves\n",
    "    split_curve_in_dir = []\n",
    "    transits_per_sector = []\n",
    "    csv_in_dir, sector_list = _fits(exoplanet, exo_folder=exo_folder, cache=cache, \n",
    "                                    hlsp=hlsp, cadence=cadence, bitmask=bitmask)\n",
    "    for i, csvfile in enumerate(csv_in_dir):\n",
    "        split_curves = split_lightcurve_file(csvfile, t0=t0, P=P, t14=t14,\n",
    "                                             cutoff=cutoff, window=window)\n",
    "        split_curves = [s + '.csv' for s in split_curves]\n",
    "        split_curve_in_dir.append(split_curves)\n",
    "        transits_per_sector.append(len(split_curves))\n",
    "    split_curve_in_dir = [i for sub in split_curve_in_dir for i in sub]\n",
    "    print(f'\\nA total of {len(split_curve_in_dir)} lightcurves '\n",
    "          'were generated.')\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Sort the files into ascending order and take random sample\n",
    "    curves = ceil(curve_sample * len(split_curve_in_dir))\n",
    "    #split_curve_in_dir = random.sample(split_curve_in_dir, k=int(curves))\n",
    "    #split_curve_in_dir = natsorted(split_curve_in_dir)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Enforce batches be the same size\n",
    "    if fit_ttv==True:\n",
    "        equal_batches = [5+3*n for n in range(0,500)]\n",
    "    else:\n",
    "        equal_batches = [6+4*n for n in range(0,500)]\n",
    "    if int(curves) > 5:\n",
    "        if int(curves) not in equal_batches:\n",
    "            import bisect\n",
    "            index = bisect.bisect(equal_batches, int(curves))\n",
    "            new_curves = equal_batches[index-1]\n",
    "            print(f'\\nEnforcing lightcurves to be in equal batch sizes. Discarded {curves-new_curves}.')\n",
    "            curves = new_curves\n",
    "    split_curve_in_dir = random.sample(split_curve_in_dir, k=int(curves))\n",
    "    split_curve_in_dir = natsorted(split_curve_in_dir)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Set the Data Paths\n",
    "    data_path = f'{exo_folder}/data_paths.csv'\n",
    "    cols = ['Path', 'Telescope', 'Filter', 'Epochs', 'Detrending']\n",
    "    df = DataFrame(columns=cols)\n",
    "    for i, split_curve in enumerate(split_curve_in_dir):\n",
    "        df = df.append([{'Path': split_curve}], ignore_index=True)\n",
    "        df['Telescope'], df['Filter'], df['Detrending'] = 0, 0, 0\n",
    "        df['Epochs'] = range(0, len(df))\n",
    "    if curve_sample==1:\n",
    "        print(f'\\nA total of {len(df)} lightcurves will be fitted'\n",
    "          ' across all TESS Sectors.\\n')\n",
    "    else:\n",
    "        print(f'\\nA random sample of {len(df)} lightcurves will be fitted'\n",
    "            ' across all TESS Sectors.\\n')\n",
    "    df.to_csv(data_path, index=False, header=True)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Paths to data, priors, and filter info:\n",
    "    data = data_path\n",
    "    priors_csv = f'{exo_folder}/{exoplanet} Priors.csv'\n",
    "    filters = 'firefly/data/TESS_filter_path.csv'\n",
    "    os.makedirs('firefly/data/ldtk', exist_ok=True)\n",
    "    ldtk_cache = 'firefly/data/ldtk'\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Output folders\n",
    "    results_output_folder = f'{exo_folder}/output_parameters'\n",
    "    fitted_lightcurve_folder = f'{exo_folder}/fitted_lightcurves'\n",
    "    plot_folder = f'{exo_folder}/plots'\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Run the retrieval\n",
    "    run_retrieval(\n",
    "            data,\n",
    "            priors_csv,\n",
    "            filters,\n",
    "            host_T=host_T,\n",
    "            host_logg=host_logg,\n",
    "            host_z=host_z,\n",
    "            host_r=host_r,\n",
    "            cadence=cadence/60,\n",
    "            walks=walks,\n",
    "            slices=slices,\n",
    "            nlive=nlive,\n",
    "            # fit_ttv=fit_ttv,\n",
    "            allow_ttv=fit_ttv,\n",
    "            detrending_list=detrending_list,\n",
    "            dynesty_sample=dynesty_sample,\n",
    "            fitting_mode=fitting_mode,\n",
    "            limb_darkening_model=limb_darkening_model,\n",
    "            ld_fit_method=ld_fit_method,\n",
    "            ldtk_cache=ldtk_cache,\n",
    "            max_batch_parameters=max_batch_parameters,\n",
    "            batch_overlap=batch_overlap,\n",
    "            dlogz=dlogz,\n",
    "            maxiter=maxiter,\n",
    "            maxcall=maxcall,\n",
    "            dynesty_bounding=dynesty_bounding,\n",
    "            normalise=normalise,\n",
    "            detrend=detrend,\n",
    "            detrending_limits=detrending_limits,\n",
    "            results_output_folder=results_output_folder,\n",
    "            final_lightcurve_folder=fitted_lightcurve_folder,\n",
    "            plot_folder=plot_folder,\n",
    "            plot=plot,\n",
    "            marker_color=marker_color,\n",
    "            line_color=line_color,\n",
    "            bin_data=bin_data,\n",
    "            binned_color=binned_color,\n",
    "            n_procs=nprocs,\n",
    "        )\n",
    "    os.makedirs('firefly/plots', exist_ok=True)\n",
    "    os.makedirs('firefly/plots/folded', exist_ok=True)\n",
    "    os.makedirs('firefly/plots/density', exist_ok=True)\n",
    "    os.makedirs('firefly/plots/densitynoerr', exist_ok=True)\n",
    "    os.makedirs('firefly/plots/oc', exist_ok=True)\n",
    "    try:\n",
    "        copy(f'{exo_folder}/plots/folded_curves/with_errorbars/filter_0.png',\n",
    "             f'firefly/plots/folded/{exoplanet}.png')\n",
    "    except:\n",
    "        print(f'Unable to overwrite {exoplanet}.jpg')\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Save Best Values\n",
    "    master = read_csv(f'{exo_folder}/output_parameters/Complete_results.csv',\n",
    "                      index_col='Parameter')\n",
    "    master = master[['Best', 'Error']]\n",
    "    P = master['Best']['P']\n",
    "    q0 = master['Best']['q0']\n",
    "    q0err = master['Error']['q0']\n",
    "    q1 = master['Best']['q1']\n",
    "    q1err = master['Error']['q1']\n",
    "    u0 = master['Best']['u0']\n",
    "    u0err = master['Error']['u0']\n",
    "    u1 = master['Best']['u1']\n",
    "    u1err = master['Error']['u1']\n",
    "    try:\n",
    "        Perr = float(master['Error'].filter(like = 'P', axis=0))\n",
    "    except ValueError:\n",
    "        Perr = float()\n",
    "    if fit_ttv==True:\n",
    "        t0 = master['Best']['t0'][0]\n",
    "        t0err = float(master['Error']['t0'][0])\n",
    "    else:\n",
    "        t0 = master['Best']['t0']\n",
    "        t0err = float(master['Error']['t0'])\n",
    "    a = master['Best']['a/AU']\n",
    "    aerr = float(master['Error']['a/AU'])\n",
    "    ar = master['Best']['a/r*']\n",
    "    arerr = float(master['Error']['a/r*'])\n",
    "    rp = master['Best']['rp/r*']\n",
    "    rperr = float(master['Error']['rp/r*'])\n",
    "    inc = master['Best']['inc']\n",
    "    incerr = float(master['Error']['inc'])\n",
    "    ecc = master['Best']['ecc']\n",
    "    try:\n",
    "        eccerr = float(master['Error']['ecc'])\n",
    "    except ValueError:\n",
    "        eccerr = float()\n",
    "    w = master['Best']['w']\n",
    "    try:\n",
    "        werr = float(master['Error']['w'])\n",
    "    except ValueError:\n",
    "        werr = float()\n",
    "    try:\n",
    "        mad, madbin, obs_depth, cadences = density_scatter(exoplanet=exoplanet,\n",
    "                                transits=int(len(df)), P=P, cadence=cadence)\n",
    "        copy(f'{exo_folder}/{exoplanet} density.png',\n",
    "             f'firefly/plots/density/{exoplanet}.png')\n",
    "        copy(f'{exo_folder}/{exoplanet} density noerr.png',\n",
    "             f'firefly/plots/densitynoerr/{exoplanet}.png')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    t_depth = rp**2\n",
    "    sens = t_depth/mad\n",
    "    sens_bin = t_depth/madbin\n",
    "    obs_sens = obs_depth/mad\n",
    "    obs_sens_bin = obs_depth/madbin\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    # Archive and sort\n",
    "    print(\n",
    "        'Variables used:\\n\\n'\n",
    "        f'target={exoplanet}\\n'\n",
    "        f'archive={archive}\\n'\n",
    "        f'hlsp={hlsp}\\n'\n",
    "        f'curve_sample={str(curve_sample)}\\n'\n",
    "        f'clean={clean}\\n'\n",
    "        f'cache={cache}\\n'\n",
    "        f'cutoff={str(cutoff)}\\n'\n",
    "        f'window={str(window)}\\n'\n",
    "        f'nlive={str(nlive)}\\n'\n",
    "        f'fit_ttv={fit_ttv}\\n'\n",
    "        f'detrending_list={str(detrending_list)}\\n'\n",
    "        f'dynesty_sample={dynesty_sample}\\n'\n",
    "        f'fitting_mode={fitting_mode}\\n'\n",
    "        f'limb_darkening_model={limb_darkening_model}\\n'\n",
    "        f'ld_fit_method={ld_fit_method}\\n'\n",
    "        f'max_batch_parameters={str(max_batch_parameters)}\\n'\n",
    "        f'batch_overlap={str(batch_overlap)}\\n'\n",
    "        f'dlogz={str(dlogz)}\\n'\n",
    "        f'maxiter={str(maxiter)}\\n'\n",
    "        f'maxcall={str(maxcall)}\\n'\n",
    "        f'dynesty_bounding={dynesty_bounding}\\n'\n",
    "        f'normalise={normalise}\\n'\n",
    "        f'detrend={detrend}',\n",
    "        file=open(exo_folder+'/variables.txt', 'w')\n",
    "    )\n",
    "    now = datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\")\n",
    "    sci_prod = ' '.join(hlsp)\n",
    "    archive_name = f\"{exoplanet} {archive.upper()} {sci_prod} {now}\"\n",
    "    if clean==True:\n",
    "        try:\n",
    "            rmtree(f'{exo_folder}/output_parameters/quicksaves')\n",
    "            rmtree(f'{exo_folder}/output_parameters/filter_0_parameters/quicksaves')\n",
    "        except Exception:\n",
    "            pass\n",
    "    if fit_ttv==True:\n",
    "        try:\n",
    "            here = os.path.dirname(os.path.abspath(__file__))\n",
    "            spearnet_csv = f'{here}/data/spear.csv'\n",
    "            spearnet = read_csv(spearnet_csv).set_index('pl_name')\n",
    "            s = spearnet.loc[[exoplanet]]\n",
    "            t0ttv = s['pl_tranmid'][0]\n",
    "            t0errttv = s['pl_tranmiderr1'][0]\n",
    "            file = f'firefly/{exoplanet}/output_parameters/Complete_results.csv'\n",
    "            chi2_redlong, nsiglong, losslong, faplong, periodlong = oc_fold(t0ttv, t0errttv,\n",
    "                                                        file=file, P=P,\n",
    "                                                        exoplanet=exoplanet)\n",
    "            try:\n",
    "                chi2_red, nsig, loss, fap, period = oc_fold(t0ttv, t0errttv,\n",
    "                                                        file=file, P=P,\n",
    "                                                        exoplanet=exoplanet,\n",
    "                                                        longterm=False)\n",
    "            except:\n",
    "                chi2_red, nsig, loss, fap, period = float(), float(), float(), float()\n",
    "                pass\n",
    "            copy(f'{exo_folder}/{exoplanet} o-c longterm.jpg',\n",
    "                 f'firefly/plots/oc/{exoplanet} long.jpg')\n",
    "            copy(f'{exo_folder}/{exoplanet} o-c.jpg',\n",
    "                 f'firefly/plots/oc/{exoplanet}.jpg')\n",
    "            data = {'pl_name':exoplanet, 'red_chi2':chi2_red, 'sigma':nsig,\n",
    "                    'mean_avg_err':loss,\n",
    "                    'fap':fap, 'o-c_period':period,\n",
    "                    'faplong':faplong, 'o-c_periodlong':periodlong,\n",
    "                    'pl_orbper':P, 'pl_orbpererr1':Perr,\n",
    "                    'pl_tranmid':t0, 'pl_tranmiderr1':t0err,\n",
    "                    'pl_orbsmax':a, 'pl_orbsmaxerr1':aerr, 'pl_radj':rp,\n",
    "                    'pl_radjerr1':rperr, 'pl_orbincl':inc,\n",
    "                    'pl_orbinclerr1':incerr, 'pl_orbeccen':ecc, 'pl_orbeccenerr1':eccerr,\n",
    "                    'pl_orblper':w, 'pl_orblpererr1':werr, 'q0':q0, 'q0err':q0err,\n",
    "                    'q1':q1, 'q1err':q1err, 'u0':u0, 'u0err':u0err,\n",
    "                    'u1':u1, 'u1err':u1err,\n",
    "                    'Transits':int(len(df)),\n",
    "                    'Date':now, 'Archive':archive.upper(), 'Unbinned Sigma':mad,\n",
    "                    'Binned Sigma':madbin,\n",
    "                'Transit Depth':t_depth, 'Sensitivity':sens,\n",
    "                'Binned Sensitivity':sens_bin,\n",
    "                'Observation Depth':obs_depth, 'Observation Sensitivity':obs_sens,\n",
    "                'Binned Observation Sensitivity':obs_sens_bin, 'Total Cadences':cadences\n",
    "            }\n",
    "            df = DataFrame(data, index=[0])\n",
    "            summary_master = 'firefly/data/spear_ttv.csv'\n",
    "            if not os.path.exists(summary_master):\n",
    "                df.to_csv(summary_master, index=False)\n",
    "            else:\n",
    "                add = read_csv(summary_master)\n",
    "                add = add.append(df)\n",
    "                add['pl_name'] = \\\n",
    "                    Categorical(add['pl_name'],\n",
    "                    ordered=True,\n",
    "                    categories=natsorted(add['pl_name'].unique()))\n",
    "                add = add.sort_values('pl_name')\n",
    "                add .to_csv(summary_master, index=False)\n",
    "        except Exception as e:\n",
    "            print(e, file=open(exo_folder+'/ttv_traceback.txt', 'w'))\n",
    "        os.makedirs('firefly/ttv', exist_ok=True)\n",
    "        os.makedirs(f'firefly/ttv/{fitting_mode}', exist_ok=True)\n",
    "        archive_folder = f'firefly/ttv/{fitting_mode}/{archive_name}'\n",
    "        make_archive(archive_folder, format='zip',\n",
    "                     root_dir=f'{os.getcwd()}/firefly/',\n",
    "                     base_dir=f'{exoplanet}')\n",
    "    else:\n",
    "        data = {'pl_name':exoplanet, 'pl_orbper':P, 'pl_orbpererr1':Perr,\n",
    "                'pl_tranmid':t0, 'pl_tranmiderr1':t0err,\n",
    "                'pl_orbsmax':a, 'pl_orbsmaxerr1':aerr, 'pl_radj':rp,\n",
    "                'pl_radjerr1':rperr, 'pl_orbincl':inc,\n",
    "                'pl_orbinclerr1':incerr, 'pl_orbeccen':ecc, 'pl_orbeccenerr1':eccerr,\n",
    "                'pl_orblper':w, 'pl_orblpererr1':werr, 'q0':q0, 'q0err':q0err,\n",
    "                'q1':q1, 'q1err':q1err, 'u0':u0, 'u0err':u0err,\n",
    "                    'u1':u1, 'u1err':u1err,\n",
    "                'Transits':int(len(df)),\n",
    "                'Date':now, 'Archive':archive.upper(), 'Unbinned Sigma':mad,\n",
    "                'Binned Sigma':madbin,\n",
    "                'Transit Depth':t_depth, 'Sensitivity':sens,\n",
    "                'Binned Sensitivity':sens_bin,\n",
    "                'Observation Depth':obs_depth, 'Observation Sensitivity':obs_sens,\n",
    "                'Binned Observation Sensitivity':obs_sens_bin, 'Total Cadences':cadences\n",
    "        }\n",
    "        df = DataFrame(data, index=[0])\n",
    "        summary_master = 'firefly/data/spear.csv'\n",
    "        if not os.path.exists(summary_master):\n",
    "            df.to_csv(summary_master, index=False)\n",
    "        else:\n",
    "            add = read_csv(summary_master)\n",
    "            add = add.append(df)\n",
    "            add['pl_name'] = \\\n",
    "                Categorical(add['pl_name'],\n",
    "                ordered=True,\n",
    "                categories=natsorted(add['pl_name'].unique()))\n",
    "            add = add.sort_values('pl_name')\n",
    "            add .to_csv(summary_master, index=False)\n",
    "        os.makedirs(f'firefly/{fitting_mode}', exist_ok=True)\n",
    "        archive_folder = f'firefly/{fitting_mode}/{archive_name}'\n",
    "        make_archive(archive_folder, format='zip',\n",
    "                     root_dir=f'{os.getcwd()}/firefly/',\n",
    "                     base_dir=f'{exoplanet}')\n",
    "    rmtree(f'{exo_folder}')\n",
    "    return archive_name\n",
    "\n",
    "\n",
    "\n",
    "def spearnet_archive_ld_params(source='13_April_2021_folded_uncoupled'):\n",
    "    import zipfile\n",
    "    import pandas as pd\n",
    "    # Find Files\n",
    "    zip_in_dir = []\n",
    "    for r, d, f in os.walk(source):\n",
    "        for item in f:\n",
    "            if '.zip' in item:\n",
    "                zip_in_dir.append(os.path.join(r, item))\n",
    "    \n",
    "    # Open Files\n",
    "    for i, zf in enumerate(zip_in_dir):\n",
    "        with zipfile.ZipFile(zf) as zip:\n",
    "            exoplanet = zip.namelist()[0].replace('/', '')\n",
    "            with zip.open(f'{exoplanet}/output_parameters/Complete_results.csv') as myZip:\n",
    "                df = pd.read_csv(myZip, index_col='Parameter')\n",
    "                master = df[['Best', 'Error']]\n",
    "                q0 = master['Best']['q0']\n",
    "                q0err = master['Error']['q0']\n",
    "                q1 = master['Best']['q1']\n",
    "                q1err = master['Error']['q1']\n",
    "                u0 = master['Best']['u0']\n",
    "                u0err = master['Error']['u0']\n",
    "                u1 = master['Best']['u1']\n",
    "                u1err = master['Error']['u1']\n",
    "                \n",
    "                data = {'pl_name':exoplanet, 'q0':q0, 'q0err':q0err,\n",
    "                        'q1':q1, 'q1err':q1err, 'u0':u0, 'u0err':u0err,\n",
    "                        'u1':u1, 'u1err':u1err}\n",
    "                df = DataFrame(data, index=[0])\n",
    "                summary_master = 'spear.csv'\n",
    "                if not os.path.exists(summary_master):\n",
    "                    df.to_csv(summary_master, index=False)\n",
    "                else:\n",
    "                    add = read_csv(summary_master)\n",
    "                    add = add.append(df)\n",
    "                    add['pl_name'] = \\\n",
    "                        Categorical(add['pl_name'],\n",
    "                        ordered=True,\n",
    "                        categories=natsorted(add['pl_name'].unique()))\n",
    "                    add = add.sort_values('pl_name')\n",
    "                    add .to_csv(summary_master, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_products(tic_id):\n",
    "   \n",
    "    search = obs.query_criteria(target_name=tic_id).to_pandas()\n",
    "    search = search[search['Cadence']==120]\n",
    "    if len(search) is None:\n",
    "        return 0\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tess(archive='nasa'):\n",
    "    #_download_archive()\n",
    "    _load_csv()\n",
    "    here = os.path.dirname(os.path.abspath(__file__))\n",
    "    mast_csv = f'{here}/data/Search/TESS_lc.csv.xz'\n",
    "    mast = read_csv(mast_csv)\n",
    "    exo_list = exo_nasa[['pl_name', 'tic_id']] \\\n",
    "              .dropna() .drop_duplicates('pl_name') \\\n",
    "              .drop(['tic_id'], axis=1) .values .tolist()\n",
    "    exo_list = [j for i in exo_list for j in i]\n",
    "    exo_list = natsorted(exo_list)\n",
    "    viable = []\n",
    "    products = []\n",
    "    period = []\n",
    "    epochs = []\n",
    "    tic = []\n",
    "    tess_mag = []\n",
    "    ra_list, dec_list, dist_list = [], [], []\n",
    "    for i, exoplanet in enumerate(exo_list):\n",
    "        try:\n",
    "            if archive=='nasa':\n",
    "                host_T, host_z, host_r, host_logg, t0, P, t14, repack, ra, dec, dist, t_mag = \\\n",
    "                            priors(exoplanet, archive, user=False)\n",
    "            else:\n",
    "                host_T, host_z, host_r, host_logg, t0, P, t14, repack = \\\n",
    "                            priors(exoplanet, archive, user=False)\n",
    "            lc_links, tic_id = _lc(exoplanet, mast)\n",
    "            if not len(lc_links)==0:\n",
    "                epoch = ceil((0.8 * 27.4 / P) * len(lc_links))\n",
    "                viable.append(exoplanet)\n",
    "                tic.append(tic_id)\n",
    "                products.append(len(lc_links))\n",
    "                period.append(P)\n",
    "                epochs.append(epoch)\n",
    "                if archive=='nasa':\n",
    "                    ra_list.append(ra)\n",
    "                    dec_list.append(dec)\n",
    "                    dist_list.append(dist)\n",
    "                    tess_mag.append(t_mag)\n",
    "            else:\n",
    "                pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    if archive=='nasa':\n",
    "        data = {'Exoplanet':viable, 'TIC ID':tic, 'Products':products, 'Period':period,\n",
    "                'Epochs':epochs, 'RA':ra_list, 'DEC':dec_list, 'Distance':dist_list,\n",
    "                'TESS Magnitude':tess_mag}\n",
    "    else:\n",
    "        data = {'Exoplanet':viable, 'TIC ID':tic, 'Products':products, 'Period':period,\n",
    "                'Epochs':epochs}\n",
    "    df = DataFrame(data)\n",
    "    df['Exoplanet'] = \\\n",
    "            Categorical(df['Exoplanet'],\n",
    "            ordered=True,\n",
    "            categories=natsorted(df['Exoplanet'].unique()))\n",
    "    df = df.sort_values('Exoplanet')\n",
    "    here = os.path.dirname(os.path.abspath(__file__))\n",
    "    df.to_csv(f'{here}/data/Targets/{archive}_tess_viable.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def gen_tess():\n",
    "    '''Generates all TESS targets which have a fitsfile.'''\n",
    "    def count_products(tic_id):\n",
    "        '''Counts the amount of TESS fitsfiles on MAST.'''\n",
    "        time.sleep(0.01)\n",
    "        search = obs.query_criteria(dataproduct_type=['timeseries'],\n",
    "                                    project='TESS',\n",
    "                                    provenance_name=['SPOC'],\n",
    "                                    t_exptime=120,\n",
    "                                    target_name=tic_id).to_pandas()\n",
    "        search = search[search['t_exptime']==120]\n",
    "        search = search[~search.dataURL.str.endswith('_dvt.fits')]\n",
    "        if len(search) is None:\n",
    "            return 0\n",
    "        return len(search)\n",
    "        \n",
    "    df = read_csv('firefly/data/nasa.csv.gz').dropna(subset=['tic_id']).reset_index(drop=True)\n",
    "    df['tic_id'] = df['tic_id'].str.replace('TIC ', '')\n",
    "    #df['Products'] = [count_products(df['tic_id'][row]) for row in range(len(df))]\n",
    "    df['Products'] = df['tic_id'].apply(count_products)\n",
    "    df['Transits'] = np.ceil((0.8 * 27.4 / df['pl_orbper']) * df['Products'])\n",
    "    df['Transits'] = df['Transits'].fillna(-1).astype(int)\n",
    "    df['pl_name'] = \\\n",
    "            Categorical(df['pl_name'],\n",
    "            ordered=True,\n",
    "            categories=natsorted(df['pl_name'].unique()))\n",
    "    df = df.sort_values('pl_name')\n",
    "    df.to_csv('nasa.csv.xz', index=False)\n",
    "    #here = os.path.dirname(os.path.abspath(__file__))\n",
    "    #df.to_csv(f'{here}/data/Targets/ML_nasa_tess_viable.csv.xz', index=False)\n",
    "\n",
    "gen_tess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ph/9w0w89cd5pq1d7z1_kn2mt8m0000gp/T/ipykernel_39686/393664935.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexo_nasa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pl_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m             Categorical(exo_nasa['pl_name'],\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             categories=natsorted(exo_nasa['pl_name'].unique()))\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#exo_nasa.to_csv('nasa.csv.xz', index=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0man\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m         \"\"\"\n\u001b[0;32m-> 1863\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/arrays/_mixins.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e134e05457d34029b6460cd73bbf1ed73f339b5b6d98c95be70b69eba114fe95"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
